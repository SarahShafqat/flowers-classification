\relax 
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\citation{Nilsback08}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Datasets}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training images and their segmentation\relax }}{2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_dataset}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distribution of the samples for each category in each dataset\relax }}{2}\protected@file@percent }
\newlabel{fig:dist}{{2}{2}}
\citation{5206848}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Methodological Approach}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Technology and implementation}{3}\protected@file@percent }
\newlabel{sec:tech}{{3.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Augmentation and Preprocessing}{3}\protected@file@percent }
\citation{kiefer1952}
\citation{kingma2014adam}
\citation{smith2015cyclical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyper-parameters}{4}\protected@file@percent }
\citation{he2015deep}
\citation{szegedy2015going}
\citation{tan2019efficientnet}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Output of the learning rate finder algorithm. $I=[10^{-5}, 10^{-3}]$ is the optimal solution\relax }}{5}\protected@file@percent }
\newlabel{fig:lr}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of the learning rate with \emph  {Triangular} and \emph  {Triangular2} methods\relax }}{5}\protected@file@percent }
\newlabel{fig:triangular}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiment 1: Choice of the architecture}{6}\protected@file@percent }
\newlabel{sec:exp1}{{3.4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces High level architecture of the network: in grey the pretrained network used as feature extractor, followed by the classifier's layers.\relax }}{6}\protected@file@percent }
\newlabel{fig:architecture}{{5}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Fine-tuning of ResNet-18}{7}\protected@file@percent }
\newlabel{sec:resnet18}{{3.4.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Fine-tuning of InceptionV3}{7}\protected@file@percent }
\newlabel{sec:inceptionv3}{{3.4.2}{7}}
\citation{Selvaraju_2019}
\citation{simonyan2013deep}
\citation{Stehman1997}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Fine-tuning of EfficientNetB4}{8}\protected@file@percent }
\newlabel{sec:efficientnetb4}{{3.4.3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Evaluation of the training}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}grad-CAM}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textit  {grad-CAM} of multiple flowers extracted from EfficientNetB4\relax }}{8}\protected@file@percent }
\newlabel{fig:grad_multi}{{6}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Saliency Map}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Example of \textit  {Saliency Map} extracted from EfficientNetB4 of a flower with an obstacle in front of it\relax }}{9}\protected@file@percent }
\newlabel{fig:sal}{{7}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Experiment 2: Freezing of the layers}{9}\protected@file@percent }
\newlabel{sec:exp2}{{3.6}{9}}
\citation{tan2019efficientnet}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Evaluation}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1}{10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax }}{10}\protected@file@percent }
\newlabel{tab:res}{{1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2}{10}\protected@file@percent }
\newlabel{sec:exp2}{{4.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confusion matrix of EfficientNetB4 on the test dataset\relax }}{11}\protected@file@percent }
\newlabel{fig:cm}{{8}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{11}\protected@file@percent }
\citation{Ashia}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Plot of the accuracy compared to the number of trainable parameters (a) and compared to training time (b) as the number of frozen layers varies\relax }}{12}\protected@file@percent }
\newlabel{fig:time_acc}{{9}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{12}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textit  {EfficientNetB4}: Comparisong between accuracy, trainable parameters and training time as the number of frozen layers increased \relax }}{12}\protected@file@percent }
\newlabel{tab:frozen}{{2}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{13}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{Nilsback08}{1}
\bibcite{5206848}{2}
\bibcite{kiefer1952}{3}
\bibcite{kingma2014adam}{4}
\bibcite{smith2015cyclical}{5}
\bibcite{he2015deep}{6}
\bibcite{szegedy2015going}{7}
\bibcite{tan2019efficientnet}{8}
\bibcite{Selvaraju_2019}{9}
\bibcite{simonyan2013deep}{10}
\bibcite{Stehman1997}{11}
\bibcite{Ashia}{12}
\@writefile{toc}{\contentsline {section}{\numberline {A}Exerimental results}{16}\protected@file@percent }
\newlabel{sec:app}{{A}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Plot of the accuracy achieved by the models during the training phase: (a) \textit  {ResNet-18}, (b) \textit  {InceptionV3}, (c) \textit  {EfficientNetB4} and (d) \textit  {EfficientNetB4} with $30\%$ of untrained layers\relax }}{16}\protected@file@percent }
\newlabel{fig:asd}{{10}{16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{16}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results of the fine-tuning of \textit  {ResNet-18}\relax }}{17}\protected@file@percent }
\newlabel{tab:resnet}{{3}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results of the fine-tuning of \textit  {InceptionV3}\relax }}{17}\protected@file@percent }
\newlabel{tab:inception}{{4}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results of the fine-tuning of \textit  {EfficientNetB4}\relax }}{18}\protected@file@percent }
\newlabel{tab:efficient}{{5}{18}}
